{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization in PyTorch Lightning with Optuna\n",
    "\n",
    "In notebook 4 of this exercise, you've learned how to develop and train models with PyTorch Lightning.\n",
    "\n",
    "In this optional notebook, we'll show you how you can perform hyperparameter optimization in PyTorch Lightning using the  framework *Optuna*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "#!pip install pytorch-lightning==0.7.6 > /dev/null\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from exercise_code.MyPytorchModel import MyPytorchModel\n",
    "from exercise_code.Util import save_model, load_model, test_and_save\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://raw.githubusercontent.com/optuna/optuna/master/docs/image/optuna-logo.png></img>\n",
    "\n",
    "Optuna is an automatic hyperparameter optimization framework, which works with several deep learning libraries, including PyTorch Lightning. Have a look at https://github.com/optuna/optuna!\n",
    "\n",
    "Two important concepts of Optuna are the terms **Study** and **Trial**:\n",
    "* **Study**: optimization based on an objective function (e.g.: maximize validation accuracy)\n",
    "* **Trial**: a single execution of the objective function (i.e., train a model for a specific hyperparameter configuration)\n",
    "\n",
    "The goal of a study is to find out the optimal set of hyperparameter values through multiple trials (e.g., n_trials=100). Optuna is a framework designed for the automation and the acceleration of the optimization studies.\n",
    "\n",
    "On a high level, hyper parameter tuning with Optuna works very similar to the search algorithms we implemented in exercise 6. However, Optuna has a set of advantages:\n",
    "\n",
    "* **Parallelization** of hyperparameter searches over multiple threads or processes without modifying code\n",
    "* more **efficient search algorithms** for large search spaces and **pruning of unpromising trials** for faster results\n",
    "* many additional features for automated search of optimal hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install & Import Optuna\n",
    "\n",
    "If you haven't done yet, install Optuna via pip by uncommenting the line in the cell below: `!pip install optuna`.\n",
    "\n",
    "If you want to install Optuna with conda or you have problems with the installation via pip, you can also use `$ conda install -c conda-forge optuna`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-1.5.0.tar.gz (200 kB)\n",
      "Collecting alembic\n",
      "  Downloading alembic-1.4.2.tar.gz (1.1 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting cliff\n",
      "  Downloading cliff-3.2.0-py3-none-any.whl (80 kB)\n",
      "Collecting cmaes>=0.5.0\n",
      "  Downloading cmaes-0.5.0-py3-none-any.whl (13 kB)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-4.1.0-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\arpita.paul31\\miniconda3\\lib\\site-packages (from optuna) (0.14.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\arpita.paul31\\miniconda3\\lib\\site-packages (from optuna) (1.18.5)\n",
      "Requirement already satisfied: scipy!=1.4.0 in c:\\users\\arpita.paul31\\miniconda3\\lib\\site-packages (from optuna) (1.4.1)\n",
      "Collecting sqlalchemy>=1.1.0\n",
      "  Downloading SQLAlchemy-1.3.17-cp37-cp37m-win_amd64.whl (1.2 MB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\arpita.paul31\\miniconda3\\lib\\site-packages (from optuna) (4.42.1)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\arpita.paul31\\miniconda3\\lib\\site-packages (from alembic->optuna) (2.8.1)\n",
      "Collecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.1.3-py2.py3-none-any.whl (75 kB)\n",
      "Collecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.4.5-py2.py3-none-any.whl (110 kB)\n",
      "Collecting cmd2!=0.8.3,<0.9.0,>=0.8.0\n",
      "  Downloading cmd2-0.8.9-py2.py3-none-any.whl (53 kB)\n",
      "Requirement already satisfied: pyparsing>=2.1.0 in c:\\users\\arpita.paul31\\miniconda3\\lib\\site-packages (from cliff->optuna) (2.4.7)\n",
      "Collecting stevedore>=1.20.0\n",
      "  Downloading stevedore-2.0.0-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\arpita.paul31\\miniconda3\\lib\\site-packages (from cliff->optuna) (1.14.0)\n",
      "Requirement already satisfied: PyYAML>=3.12 in c:\\users\\arpita.paul31\\miniconda3\\lib\\site-packages (from cliff->optuna) (5.3.1)\n",
      "Collecting PrettyTable<0.8,>=0.7.2\n",
      "  Downloading prettytable-0.7.2.tar.bz2 (21 kB)\n",
      "Requirement already satisfied: colorama; sys_platform == \"win32\" in c:\\users\\arpita.paul31\\miniconda3\\lib\\site-packages (from colorlog->optuna) (0.4.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\arpita.paul31\\miniconda3\\lib\\site-packages (from Mako->alembic->optuna) (1.1.1)\n",
      "Collecting pyperclip\n",
      "  Downloading pyperclip-1.8.0.tar.gz (16 kB)\n",
      "Collecting pyreadline; sys_platform == \"win32\"\n",
      "  Downloading pyreadline-2.1.zip (109 kB)\n",
      "Building wheels for collected packages: optuna, alembic, PrettyTable, pyperclip, pyreadline\n",
      "  Building wheel for optuna (setup.py): started\n",
      "  Building wheel for optuna (setup.py): finished with status 'done'\n",
      "  Created wheel for optuna: filename=optuna-1.5.0-py3-none-any.whl size=276148 sha256=25796e0dfa5fefb5a62b86680554a3d2ce27890083f76d633e3a04d6f101c3b8\n",
      "  Stored in directory: c:\\users\\arpita.paul31\\appdata\\local\\pip\\cache\\wheels\\0a\\f9\\1b\\1c068c5fb648ad0a4600c104495c61ae780a2f3bcf52bcd676\n",
      "  Building wheel for alembic (PEP 517): started\n",
      "  Building wheel for alembic (PEP 517): finished with status 'done'\n",
      "  Created wheel for alembic: filename=alembic-1.4.2-py2.py3-none-any.whl size=159547 sha256=ced694a824f63b3aac2a5893d59e5e20d3b44fe641ce66fb3a2d29b17540ce06\n",
      "  Stored in directory: c:\\users\\arpita.paul31\\appdata\\local\\pip\\cache\\wheels\\4e\\b5\\00\\f93fe1c90b3d501774e91e2e99987f49d16019e40e4bd3afc3\n",
      "  Building wheel for PrettyTable (setup.py): started\n",
      "  Building wheel for PrettyTable (setup.py): finished with status 'done'\n",
      "  Created wheel for PrettyTable: filename=prettytable-0.7.2-py3-none-any.whl size=13704 sha256=17ed05031a3c4c4b4dafd01d541ac10d30cc6162cfc438c4c744af9e737043b3\n",
      "  Stored in directory: c:\\users\\arpita.paul31\\appdata\\local\\pip\\cache\\wheels\\8c\\76\\0b\\eb9eb3da7e2335e3577e3f96a0ae9f74f206e26457bd1a2bc8\n",
      "  Building wheel for pyperclip (setup.py): started\n",
      "  Building wheel for pyperclip (setup.py): finished with status 'done'\n",
      "  Created wheel for pyperclip: filename=pyperclip-1.8.0-py3-none-any.whl size=8696 sha256=bafa4b95aa200fd8a65181daef924b01a0fcb130b394e7fea03edfe43be5e1c6\n",
      "  Stored in directory: c:\\users\\arpita.paul31\\appdata\\local\\pip\\cache\\wheels\\e5\\5e\\f7\\441179ddf6ac56f36cb1d84d94f35beedd5da15986ce3d321d\n",
      "  Building wheel for pyreadline (setup.py): started\n",
      "  Building wheel for pyreadline (setup.py): finished with status 'done'\n",
      "  Created wheel for pyreadline: filename=pyreadline-2.1-py3-none-any.whl size=93844 sha256=735f81f96276e70b1e67af9fbd4a36edc70187f9a2bad25cef943d8396a16f05\n",
      "  Stored in directory: c:\\users\\arpita.paul31\\appdata\\local\\pip\\cache\\wheels\\00\\6e\\d4\\7c4b7bc22c090baf4f470e7f35c4f22206277229bdb6607df6\n",
      "Successfully built optuna alembic PrettyTable pyperclip pyreadline\n",
      "Installing collected packages: sqlalchemy, python-editor, Mako, alembic, pbr, pyperclip, pyreadline, cmd2, stevedore, PrettyTable, cliff, cmaes, colorlog, optuna\n",
      "Successfully installed Mako-1.1.3 PrettyTable-0.7.2 alembic-1.4.2 cliff-3.2.0 cmaes-0.5.0 cmd2-0.8.9 colorlog-4.1.0 optuna-1.5.0 pbr-5.4.5 pyperclip-1.8.0 pyreadline-2.1 python-editor-1.0.4 sqlalchemy-1.3.17 stevedore-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger\n",
    "\n",
    "The default logger in PyTorch Lightning automatically writes to event files to be consumed by TensorBoard. \n",
    "\n",
    "Here we just set up a simple callback, that keeps the metrics from each validation step in memory, which we can then use to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Callback\n",
    "\n",
    "class MetricsCallback(Callback):\n",
    "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.metrics = []\n",
    "\n",
    "    def on_validation_end(self, trainer, pl_module):\n",
    "        self.metrics.append(trainer.callback_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "As mentioned above, the goal of our hyperparameter tuning is, to optimize an objective function by finding the best set of hyperparameters. \n",
    "(\n",
    "Optuna is a black-box optimizer, which means we need to provide this objective function (`objective(trial)`), which gets passed a `trial` object and returns a numerical value to evaluate the performance of the current hyperparameters and where to sample in the upcoming trial.\n",
    "\n",
    "In our case, the metric we want to optimize is the validation accuracy of our models. To get the validation accuracy for a given hyperparameter configuration, we do the same thing as in the previous notebook:\n",
    "\n",
    "* define a PL-trainer\n",
    "* define hyper parameters - by sampling them from the provided hyperparameter ranges, similar as in our old random search implementation\n",
    "* initialize a model with these hyperparameters\n",
    "* train that model, and report its validation accuracy as our objective function value\n",
    "\n",
    "As you can see in the code cell below, the implementation is straight forward:\n",
    "\n",
    "PS: notice the different sampling modes, e.g. `trial.suggest_int`, `trial.suggest_loguniform` which are also similar to our previous implementation! Check out the documentation at https://optuna.readthedocs.io/en/latest/reference/trial.html to find out about further sampling modes, and **feel free to add additional hyperparameters!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # as explained above, we'll use this callback to collect the validation accuracies\n",
    "    metrics_callback = MetricsCallback()  \n",
    "    \n",
    "    # create a trainer\n",
    "    trainer = pl.Trainer(\n",
    "        #train_percent_check=1.0,\n",
    "        #val_percent_check=1.0,\n",
    "        logger=False,                                                                  # deactivate PL logging\n",
    "        max_epochs=1,                                                                  # epochs\n",
    "        gpus=0 if torch.cuda.is_available() else None,                                 # #gpus\n",
    "        callbacks=[metrics_callback],                                                  # save latest accuracy\n",
    "        early_stop_callback=PyTorchLightningPruningCallback(trial, monitor=\"val_acc\"), # early stopping\n",
    "    )\n",
    "\n",
    "    # here we sample the hyper params, similar as in our old random search\n",
    "    trial_hparams = {\"n_hidden\": trial.suggest_int(\"n_hidden\", 100, 200), \n",
    "                     \"batch_size\": 64, \n",
    "                     \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-6, 1e-1)}\n",
    "    \n",
    "    # create model from these hyper params and train it\n",
    "    model = MyPytorchModel(trial_hparams)\n",
    "    model.prepare_data()\n",
    "    trainer.fit(model)\n",
    "\n",
    "    # save model\n",
    "    save_model(model, '{}.p'.format(trial.number), \"checkpoints\")\n",
    "\n",
    "    # return validation accuracy from latest model, as that's what we want to minimize by our hyper param search\n",
    "    return metrics_callback.metrics[-1][\"val_acc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important difference to our previous search implementation is, that Optuna does not sample randomly! \n",
    "\n",
    "It uses a **Tree-structured Parzen Estimator (TPE)** sampler, which is a kind of bayesian optimization, and more efficient than a pure random search, because it chooses the hyperparameter values after evaluating all previous trials to make a smart guess where the best hyperparameters can be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've defined our objective function, we can start the search.\n",
    "\n",
    "For that, we can also provide a **pruner**, which is a very useful concept supported by Optuna: Similar to *early stopping*, a pruner automatically stops unpromising trials in early stages of training (a.k.a. automated early-stopping) and therefore can significantly speed up the search.\n",
    "\n",
    "If you want to specify a pruner, have a look at the available options at https://optuna.readthedocs.io/en/latest/reference/pruners.html. \n",
    "\n",
    "Then, we create our `study`, define the direction that we want to optimize (i.e., *maximize* the validation accuracy) and start the optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruner = optuna.pruners.NopPruner()\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "study.optimize(objective, n_trials=2, timeout=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we save every model in a directory called `./checkpoints`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've finished the search, we access the best trial at `study.best_trial`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {}\".format(best_trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving your best model\n",
    "\n",
    "\n",
    "When you're done with your hyperparameter search and have achieved at least 50% validation accuracy, you can save your best model to the `./models`-directory in order to submit the model.\n",
    "\n",
    "Before that, we will check again whether the number of parameters is below 5 Mio and the file size is below 20 MB.\n",
    "\n",
    "When your final model is saved, we'll lastly report the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model('checkpoints/{}.p'.format(best_trial.number))\n",
    "best_model.prepare_data()\n",
    "test_and_save(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove checkpoints directory\n",
    "Lastly, let's remove the checkpoints directory again to clear up space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"checkpoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Here we provided just a quick overview about hyperparameter optimization using Optuna. Optuna has a lot more to offer, so definitely make sure to check out the following resources:\n",
    "\n",
    "* Source Code: https://github.com/optuna/optuna\n",
    "* Docs: https://optuna.readthedocs.io/en/stable/\n",
    "* Website: https://optuna.org\n",
    "* Paper: https://arxiv.org/pdf/1907.10902.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
